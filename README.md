# LLM 学习笔记

这个仓库用于记录学习大语言模型(Large Language Models, LLMs)相关知识的笔记。

## 仓库结构

```
├── 01-fundamentals/       # 基础知识
│   ├── attention/        # 注意力机制
│   ├── transformer/      # Transformer架构
│   └── tokenization/     # 分词技术
├── 02-architectures/      # 模型架构
│   ├── gpt/             # GPT系列
│   ├── bert/            # BERT及其变体
│   └── others/          # 其他架构
├── 03-training/          # 训练方法
│   ├── pre-training/    # 预训练技术
│   ├── fine-tuning/     # 微调方法
│   └── optimization/    # 优化技术
├── 04-applications/      # 应用场景
│   ├── nlp-tasks/       # NLP任务
│   └── domain-specific/ # 领域特定应用
└── 05-advanced/          # 进阶主题
    ├── scaling-laws/    # 缩放定律
    ├── alignment/       # 对齐技术
    └── efficiency/      # 效率优化
```

## 学习路线

1. **基础知识**
   - Transformer架构详解
   - 注意力机制原理
   - 分词方法与实现

2. **核心技术**
   - 预训练方法与策略
   - 模型架构演进
   - 训练技巧与优化

3. **进阶主题**
   - 模型扩展与效率
   - 对齐与安全
   - 最新研究进展

## 如何使用

- 每个主题都会包含详细的笔记和代码示例
- 推荐按照学习路线顺序学习
- 欢迎通过Issues讨论和提问

## 参考资源

- 论文阅读清单
- 推荐学习资源
- 相关项目链接

## 贡献指南

欢迎通过以下方式贡献:
1. 提交Issue讨论想法
2. 提交Pull Request完善内容
3. 分享学习经验和建议